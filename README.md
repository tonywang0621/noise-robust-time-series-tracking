# Week 1 – Data Generation and Visualization

## Motivation
In many real-world time-series problems, the true system state evolves over time but cannot be directly observed due to noise. 
Instead, we only have access to noisy measurements that partially reflect the underlying state.

Before applying any estimation or machine learning method, it is crucial to understand how such data is generated and how noise affects observations.

This week focuses on building a simple noisy dynamic system and visualizing the relationship between the hidden state and noisy observations.

---

## System Description
We consider a one-dimensional discrete-time dynamic system with the following assumptions:

### Hidden State (Process Model)
The true system state evolves as a random walk:

$x_{t+1} = x_t + w_t,\quad w_t \sim \mathcal{N}(0, Q)$


where:
- $x_t\$ is the hidden (unobserved) true state,
- $w_t\$ is the process noise,
- $Q\$ controls how much the true state fluctuates over time.

### Observation Model
The observed measurement is a noisy version of the true state:

$y_t = x_t + v_t, \quad v_t \sim \mathcal{N}(0, R)$




where:
- $y_t\$ is the observed value,
- $v_t\$ is the measurement noise,
- $R\$ controls the noise level of the observation.

---

## Data Generation
Synthetic time-series data is generated by:
1. Simulating the hidden state sequence using the random walk model.
2. Adding Gaussian measurement noise to produce noisy observations.

Both process noise variance $Q\$ and measurement noise variance $R\$ are adjustable to study their effects.

---

## Visualization
We visualize:
- The hidden true state as a continuous line.
- The noisy observations as scattered points.

This comparison highlights the key challenge of time-series tracking:
> Noisy observations do not directly reveal the true system state.

---

## Key Observations
From the visualization:
- Larger $Q\$ leads to a more volatile true state.
- Larger $R\$ results in noisier observations.
- Noisy measurements alone can be misleading without considering the underlying dynamics.

---

## Next Step
In Week 2, we will apply simple baseline methods (naive estimation, moving average, and regression-based approaches) to estimate the hidden state and evaluate their performance.

# Week 2 – Baseline Methods under Noisy Observations

## Purpose of Baselines
Before applying any advanced filtering or model-based estimation method, it is important to understand how simple and intuitive approaches behave under noisy conditions.

In this week, several baseline methods are introduced to establish reference points for comparison. These baselines help clarify the limitations of observation-only and data-driven approaches, and motivate the need for a more principled state estimation framework.

---

## Baseline Methods

### Naive Baseline
The naive baseline directly uses the noisy observation as the estimate of the system state:
- No noise handling
- No temporal modeling

This baseline represents the lower bound of performance and serves as a reference for the worst-case scenario.

---

### Moving Average Baseline
The moving average baseline smooths the noisy observations by averaging the most recent measurements within a fixed window.

- Effectively reduces measurement noise
- Introduces temporal lag when the true state changes
- Does not distinguish between noise and genuine state transitions

This method highlights the trade-off between noise reduction and responsiveness.

---

### Linear Regression Baseline
The regression-based baseline predicts the current state using a linear combination of past observations.

- Purely data-driven
- Does not assume any system dynamics
- Sensitive to measurement noise
- May overfit noisy observations

This baseline represents the simplest form of machine learning without explicit modeling of the underlying process.

---

## Key Observations
From the experimental results and visual comparisons:

- The naive baseline closely follows noisy observations and is highly sensitive to measurement noise.
- Moving average smoothing reduces noise but consistently lags behind the true state, especially when the state changes rapidly.
- Linear regression captures short-term trends but can become unstable under high noise levels and tends to track noise rather than the underlying state.

These results demonstrate that observation-based and data-driven methods alone are insufficient for reliable state estimation in noisy dynamic systems.

---

## Motivation for Next Step
The limitations observed in the baseline methods motivate the use of model-based filtering approaches.

An effective estimator should:
- Reduce measurement noise without introducing excessive lag
- Incorporate knowledge of system dynamics
- Balance trust between observations and model predictions

In Week 3, a Kalman filter will be introduced to address these challenges by explicitly modeling state evolution and uncertainty.

# Week 3 – Kalman Filter for Noisy Dynamic Systems

## Motivation
In Week 2, several baseline methods were used to estimate system states under noisy observations, including naive estimation, moving average smoothing, and linear regression.
Although these approaches provide partial improvements, they each exhibit fundamental limitations:

- Naive estimation is highly sensitive to measurement noise.
- Moving average smoothing reduces noise but introduces temporal lag.
- Linear regression is purely data-driven and may overreact to noisy observations.

These limitations motivate the use of a model-based filtering approach that explicitly accounts for system dynamics and uncertainty.

---

## System Model (Problem Setup)
We consider a one-dimensional discrete-time dynamic system.
The true system state is not directly observable and evolves over time, while observations are corrupted by noise.

The system is modeled as a **random walk**, defined as:


$x_t = x_{t-1} + w_{t-1}, \quad w_{t-1} \sim \mathcal{N}(0, Q)$


where:
- $x_t\$ denotes the true (hidden) system state at time \(t\),
- $w_{t-1}\$ represents process noise,
- $Q\$ controls the uncertainty in the system evolution.

Observations are given by:


$y_t = x_t + v_t, \quad v_t \sim \mathcal{N}(0, R)$


where:
- $y_t\$ is the noisy observation,
- $v_t\$ represents measurement noise,
- $R\$ controls the reliability of observations.

This formulation captures the core challenge of state estimation: inferring an evolving hidden state from noisy measurements.

---

## Uncertainty Interpretation (Q and R)
Two sources of uncertainty are explicitly modeled:

- **Process noise Q:**  
  Describes how unpredictable the system dynamics are.  
  A larger $Q\$ implies the state may change more rapidly between time steps.

- **Measurement noise R:**  
  Describes how unreliable the observations are.  
  A larger $R\$ implies observations contain more noise.

The relative magnitudes of $Q\$ and $R\$ determine how much the filter trusts model predictions versus new observations.

---

## Kalman Filter Formulation (1D)
Under the above assumptions, the Kalman filter recursively estimates the system state using two steps: **prediction** and **update**.

### Prediction Step
Before observing $y_t\$, the state and uncertainty are predicted as:


$\hat{x}_{t|t-1} = \hat{x}_{t-1|t-1}$


$P_{t|t-1} = P_{t-1|t-1} + Q$

where:
- $\hat{x}_{t|t-1}\$ is the predicted state estimate,
- $P_{t|t-1}\$ is the predicted uncertainty.

---

### Update Step
After receiving the new observation %y_t\$, the Kalman gain is computed:


$K_t = \frac{P_{t|t-1}}{P_{t|t-1} + R}$


The state estimate and uncertainty are then updated:


$\hat{x}_{t|t} = \hat{x}_{t|t-1} + K_t \bigl(y_t - \hat{x}_{t|t-1}\bigr)$


$P_{t|t} = (1 - K_t) P_{t|t-1}$

The Kalman gain \(K_t\) determines how much the estimate is corrected toward the observation.

---

## Comparison with Baseline Methods
Using the same random walk system and noisy observations as in Week 2:

- The Kalman filter significantly reduces sensitivity to measurement noise compared to the naive baseline.
- Unlike moving average smoothing, it avoids excessive temporal lag.
- Compared to linear regression, it produces more stable estimates by explicitly modeling uncertainty.

This demonstrates the advantage of model-based filtering over purely observation-based or data-driven methods.

---

## Sensitivity Analysis: Effects of Q and R
To further understand filter behavior, sensitivity analysis is conducted by varying the noise parameters.

### Effect of Measurement Noise (R)
As \(R\) increases:
- The Kalman gain decreases.
- The filter relies more on model predictions.
- State estimates become smoother and less sensitive to noisy observations.

### Effect of Process Noise (Q)
As \(Q\) increases:
- Predicted uncertainty increases.
- The Kalman gain increases.
- The filter responds more strongly to new observations.

---

## Key Takeaways
- The Kalman filter provides a principled solution for state estimation in noisy dynamic systems.
- Explicit modeling of uncertainty allows it to balance noise reduction and responsiveness.
- The parameters \(Q\) and \(R\) play a critical role in controlling filter behavior.

This week completes the transition from baseline methods to model-based state estimation, establishing a foundation for more advanced filtering and dynamic models.
